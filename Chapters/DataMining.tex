\chapter{Data Mining}
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.52]{images/Data_Mining_process.jpg}
    \caption{The Data Mining Process}
\end{figure}

\textbf{\textit{Data Mining}} is the process of discovering patterns, trends, correlations, or meaningful information from large datasets. It involves the application of various techniques from \textit{statistics}, \textit{machine learning}, and \textit{database systems} to extract valuable knowledge from raw data.

\section{Business Intelligence and Data Warehouses}

\paragraph{Business Intelligence (BI)}
\textbf{\textit{Business Intelligence (BI)}} represents a key concept in the field of Data Mining and can be described as the process of:
\begin{itemize}
    \item transforming raw data into useful information to support effective and aware business strategies
    \item capturing the business data and getting the right information to the right \textit{people}, at the right \textit{time}, through the right \textit{channel}.
\end{itemize}
There are different definition that has been provided during the years, but there are two of them in particular that we can highlight:
\begin{quote}
    \textit{Business intelligence (BI) is an umbrella term that includes the applications, infrastructure and tools, and best practices that enable access to and analysis of information to improve and optimize decisions and performance. - \textbf{Gartner}}
\end{quote}
\begin{quote}
    \textit{Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making. - \textbf{Forrester Research}}
\end{quote}

\paragraph{Data Warehouse (DWH)}
One of the main tools to support BI is the \textbf{\textit{Data Warehouse (DWH)}}, which is a type of \textit{Decision Support System (DSS)} and can be seen informally as an optimized repository that stores information for the decision-making process. With the huge and increasing number of information that companies have to manage in order to find relevant business strategies DWHs answer to the necessity of more sophisticated solutions than classical operational databases.\\
The main advantages are the following:
\begin{itemize}
    \item they provide the ability to manage sets of historical data;
    \item they provide the ability to run multidimensional analysis accurately and rapidly;
    \item they are based on a simple model that can be easily learned by its users;
    \item they are the basis for indicator-calculating systems.
\end{itemize}

More formally, we can say that a \textbf{\textit{Data Warehouse (DWH)}} is a specialized database that stores large volumes of historical data and facilitates the analysis and reporting of that data to support \textit{decision-making processes}. It provides several key features:

\begin{itemize}
    \item \textbf{Subject-Oriented:} A data warehouse is designed to focus on specific subjects or domains relevant to the enterprise's operations, such as customers, products, sales, etc. By organizing data around these core concepts, it enables analysts and decision-makers to gain insights into various aspects of the business.

    \item \textbf{Integration and Consistency:} One of the primary functions of a DWH is to integrate data from multiple disparate sources, including transactional databases, spreadsheets, and external systems. This integration ensures that data from different sources is harmonized and provides a unified view across the organization. Consistency in data representation and structure is maintained to ensure accuracy and reliability in analysis.

    \item \textbf{Evolution Over Time and Non-Volatility:} A crucial aspect of a data warehouse is its ability to capture and track changes in data over time. Historical data are preserved, allowing for the analysis of trends and patterns spanning various time periods. Unlike transactional databases where data may be constantly updated or overwritten, data in a data warehouse are non-volatile. Once committed, the data remains static, read-only, and preserved for future reporting and analysis.

\end{itemize}

In summary, a \textbf{\textit{DWH}} serves as a centralized repository for historical data, providing a comprehensive and consistent view of the organization's information assets. By offering subject-oriented, integrated, and non-volatile data, it empowers decision-makers with valuable insights for strategic planning, performance analysis, and informed decision-making.

\paragraph{Data Mart (DM)}
A \textbf{Data Mart (DM)} serves as a specialized subset or aggregation of the data stored within a primary DWH. Unlike the comprehensive nature of a DWH, a \textbf{DM} contains a focused set of information tailored to \textit{meet the needs of a specific business area}, corporate department, or category of users.

One of the key roles of \textbf{DMs} is to act as building blocks during the incremental development of DWHs. Rather than attempting to construct an entire DWH in one go, organizations often adopt an iterative approach, creating smaller, more \textit{targeted DMs} that address immediate business needs. As the organization's analytical requirements evolve, additional DMs can be added or expanded upon, gradually contributing to the development of a comprehensive DWH architecture.

Moreover, \textbf{DMs} help to address the users' queries more efficiently. By tailoring the data content and structure to align with the analytical needs of particular departments or user categories, DMs facilitate more efficient and focused analysis. This granularity enables users to access and analyze relevant data without being overwhelmed by the vast volume of information typically found in a primary DWH.

Furthermore, \textbf{DMs} often offer improved performance compared to primary DWHs. Due to their smaller size and targeted scope, DMs can deliver faster query response times and better overall system performance. By focusing on specific subsets of data, data marts reduce the complexity of queries and minimize the processing overhead associated with accessing and retrieving information.

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.39]{images/BI_pyramid.png}
\end{figure}

\subsection{Online Analytical Processing (OLAP)}

First of all, data are organised in \textbf{tables}, which are defined as collections of related, homogeneous data arranged into a row-column format, which includes:
\begin{itemize}
    \item \textbf{Rows:} the various components stored in the table about a \textit{specific individual};
    \item \textbf{Columns:} the type and the meaning of a \textit{specific component} of the individuals represented in the table;
    \item \textbf{Key:} a column or a set of columns whose values allow to distinguish \textit{univocally} the rows of the table.
\end{itemize}

\textbf{\textit{On-Line Analytical Processing (OLAP)}} allows users  to interactively navigate the data warehouse information exploiting the multidimensional model, providing a flexible and intuitive way to explore, query, and report on large datasets. Typically, the data are analyzed at different levels of aggregation, by applying subsequent \textit{\textbf{OLAP operators}}, each yielding one or more different queries.

In an \textbf{OLAP Session} the user can scout the multidimensional model choosing the next operator based on the outcome of the previous ones. In this way, the user creates a navigation path that corresponds to an analysis process for facts according to different points and at different detail levels.
\begin{quote}
    \centering
    \textit{Product} $\longrightarrow$ \textit{Sub-Category} $\longrightarrow$ \textit{Category}
\end{quote}

The \textbf{OLAP operators} are the following:
\begin{itemize}
    \item \textbf{Roll-up:} causes an increase in data aggregation and removes a detail level from a hierarchy by collapsing the rows that have a feature in common.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.55]{images/OLAP_roll_up.png}
    \end{figure}
    \item \textbf{Drill-down:} is the complement to the roll-up operator; it reduces data aggregation and adds a new detail level to a hierarchy (e.g., from category to subcategory).
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.55]{images/OLAP_drill_down.png}
    \end{figure}
    \item \textbf{Slice-and-dice:} the \textit{slicing} operation reduces the number of cube dimensions after setting one of the dimensions to a specific value (e.g., category = 'Food and Beverages'); the \textit{dicing} operation reduces the set of data being analysed by a selection criterion.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.55]{images/OLAP_slice_and_dice.png}
    \end{figure}
    \item \textbf{Pivot:} implies a change in layouts, aiming at analysing a group of data from a different viewpoint.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.55]{images/OLAP_pivot.png}
    \end{figure}
    \item \textbf{Drill-across:} allows to create a link between concepts in interrelated cubes, in order to compare them.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.5]{images/OLAP_drill_across.png}
    \end{figure}
    \item \textbf{Drill-through:} switches from multidimensional aggregate data to operational data insources or in the reconciled layer.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.44]{images/OLAP_drill_through.png}
    \end{figure}
\end{itemize}

\subsection{Extraction, Transformation and Loading (ETL)}

The \textit{\textbf{ETL (Extract, Transform, Load)}} process is a crucial component of data warehousing and business intelligence. It involves \textbf{extracting} data from various sources, \textbf{cleansing} them, \textbf{transforming} them into a consistent format, and \textbf{loading} them into a target destination, such as a data warehouse, where they can be analyzed and queried effectively. Let's analyse the single phases.

\subsubsection{Extraction}
In the \textbf{extraction phase}, data (\textit{structured} or \textit{unstructured}) is collected from multiple disparate sources, including databases, flat files, APIs, and external systems. The methods used for this process vary depending on the source systems and the nature of the data:
\begin{itemize}
    \item \textbf{Static extraction:} retrieving all the data from the source. Used to populate the data warehouse for the first time.
    \item \textbf{Incremental extraction:} updating the data warehouse regularly only with the data that have been modified. In this case data comes with an associated \textit{timestamp} and \textit{triggers} (related to change transactions for relevant data).
    \item \textbf{Real-Time extraction:} continous stream of data.
\end{itemize}

\subsubsection{Cleansing}
This phase is about all those procedures to improve the \textbf{quality} of the retrieved data, by standardizing it and correcting \textbf{mistakes} like \textit{duplicate or missing data}, \textit{unexpeted use of some fields}, \textit{impossible or wrong values} and \textbf{inconsistences} due to \textit{different practices used} or \textit{typing mistakes}. Each type of problem requires different techniques for its solution. We can distinguish three main techniques:
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.34]{images/ETL_dict_based_techniques.png}
    \caption{Format discrepancies}
    \label{figDictBasedTec}
\end{figure}
\begin{itemize}
    \item \textbf{Dictionary-based techniques:} they are used to check the correctness of the attribute values based on \textit{lookup tables} and \textit{dictionaries} to search for abbreviations and synonyms. We can apply these techniques if the domain is known and limited. These techniques are suitable for typing mistakes and format discrepancies [Figure \ref{figDictBasedTec}, \ref{figDictBasedTec2}].
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.43]{images/ETL_dict_based_techniques_2.png}
        \caption{Inconsistences between correlated attributes}
        \label{figDictBasedTec2}
    \end{figure}
    \item \textbf{Approximate merging:} we use this technique when we need to merge data coming from different sources and we don't have a common key to identify matching tuples:
    \begin{itemize}
        \item \textit{Approximate join} - comparing records from different datasets using similarity measures or matching algorithms to identify potentially related records. (?)
        \begin{figure}[ht!]
            \centering
            \includegraphics[scale=0.9]{images/ETL_approximate_join.png}
            \caption{Approximate join on \textit{Customer Address} and \textit{Customer Surname}}
        \end{figure}
        \item \textit{Similary functions} - usage of \textbf{affinity functions} (Levenshtein distance, Jaccard similarity, etc.) to compute the similarity between two words and if the result is higher/lower than a treshold, then the two words are the same and the rows can be merged. 
        \begin{figure}[ht!]
            \centering
            \includegraphics[scale=0.78]{images/ETL_similarity_functions.png}
        \end{figure}
    \end{itemize}
    \item \textbf{Ad-hoc algorithms:} custom algorithms based on specific business rules.
\end{itemize}

\subsubsection{Transformation}
In this phase, data from sources is properly transformed to adjust its format to the reconciled schema. It includes:
\begin{itemize}
    \item \textbf{Conversion:} changes on data
    types and format, like:
    \begin{itemize}
        \item \textit{date conversion}: from date to number (e.g. 12/11/2018 $\longrightarrow$ 20181112)
        \item \textit{string conversion}: lowercase to uppercase (e.g. unibo $\longrightarrow$ UNIBO)
        \item \textit{naming convention transformation}: short description to long
        description (e.g. IT $\longrightarrow$ Italy)
    \end{itemize}
    \item \textbf{Enrichment:} combination of one or more attribute to create new information, like derived data (e.g. Profit = Receipts - Expenses).
    \item \textbf{Separation/Concatenation:} attributes concatenation (e.g. customer surname $\Vert$ customer name)
    \item \textbf{Denormalization/Normalization:} organization of data in tables, where each piece appears only once for normalization and introduction of small redundancy to improve query performance. Typically, in the DWH the data is \textit{denormalized}.
    \begin{figure}[ht!]
        \centering
        \includegraphics[scale=0.8]{images/ETL_denormalization.png}
        \caption{Denormalization process}
    \end{figure}
\end{itemize}

\subsubsection{Loading}
This is the phase of loading data into a DWH, which can be done in batches or in real-time, depending on the volume of data. During the loading process, data integrity and consistency are maintained to ensure that the data remains accurate and usable for analysis. There are two ways of approaching this phase:
\begin{itemize}
    \item \textbf{Refresh:} the DWH is completely rewritten (i.e. older data is replaced). It's used in combination with static extraction.
    \item \textbf{Update:} only those changes applied to source data are added to the DWH. Preexisting data is not deleted or modified. It's used in combination with incremental extraction to regularly update the DWH.
\end{itemize}

\subsection{Data Warehouse Architectures}

The requirements that a Data Warehouse has to satisfy are the following:
\begin{itemize}
    \item \textbf{Separation:} analytical and transactional processing should be kept apart as much as possible.
    \item \textbf{Scalability:} hardware and software architectures should be easy to upgrade as the data volume, which has to be managed and processed, and the number of users' requirements, which have to be met, progressively increase.
    \item \textbf{Extensibility:} the architecture should be able to host new applications and technologies without redesigning the whole system.
    \item \textbf{Security:} monitoring accesses is essential because of the strategic data stored in data warehouses.
    \item \textbf{Administrability:} DWH management should not be overly difficult.
\end{itemize}

\subsubsection{Single-Layer Architecture}
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.7]{images/DWH_single_layer.png}
\end{figure}
A \textit{\textbf{Single-Layer architecture}} for a data warehouse provides a straightforward and integrated approach to data storage, processing, and presentation. The source layer is the only physically available layer and its goal is to minimize the amount of data stored, removing data redundancies. DWH is implemented as a multidimensional view of operational data created by specific \textit{middleware}.

Even if this architecture minimizes the space occupation, there is no separation between analytical and transactional processing, which is not ideal for large DWHs and for supporting complex analytical workloads efficiently.

\subsubsection{Two-Layer Architecture}


\subsubsection{Three-Layer Architecture}


\subsection{Conceptual Modeling: The Dimensional Fact Model (DFM)}

